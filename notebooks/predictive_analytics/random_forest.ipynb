{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78318c5a",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d88398",
   "metadata": {},
   "source": [
    "We chose the random forest regression for our prediction, because it can handle categorical features, provide feature importance and has less tendency to overfit. Additionally it works well with a large amount of data. A drawback can be that it is hard to understand the overall decision making process of the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480c1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "# from utils.evaluation import mean_average_percentage_error, root_mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a156ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_bike_trips_hourly = pd.read_parquet('../../data/bike_trips_hourly_FINAL.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369a1d5",
   "metadata": {},
   "source": [
    "### Define X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc49f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_bike_trips_hourly.drop(['starting_trips'], axis=1)\n",
    "y = df_bike_trips_hourly['starting_trips']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b42cb31",
   "metadata": {},
   "source": [
    "### Train the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d334928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037598ea",
   "metadata": {},
   "source": [
    "We use a grid search to find the [optimal combination of hyper-parameters](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74):\n",
    "\n",
    "`max_features` = max number of features considered for splitting a node\n",
    "\n",
    "`min_samples_leaf` = min number of data points allowed in a leaf node\n",
    "\n",
    "`min_samples_split` = min number of data points placed in a node before the node is split\n",
    "\n",
    "`max_depth` = max number of levels in each decision tree\n",
    "\n",
    "`max_leaf_nodes` = max number of leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25fb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(n_estimators=100, bootstrap=True, random_state=42)\n",
    "param_grid = {\n",
    "\t'max_features': ['auto', 'sqrt', 'log2'],\n",
    "\t'min_samples_leaf': [1, 2, 4, 8],\n",
    "\t'min_samples_split': [2, 4, 8],\n",
    "\t'max_depth': [None, 5, 10, 50, 100],\n",
    "\t'max_leaf_nodes': [None, 10, 50, 100, 150],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933ce7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 900 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n",
       "             param_grid={'max_depth': [None, 5, 10, 50, 100],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'max_leaf_nodes': [None, 10, 50, 100, 150],\n",
       "                         'min_samples_leaf': [1, 2, 4, 8],\n",
       "                         'min_samples_split': [2, 4, 8]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(\n",
    "    estimator, param_grid, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=-1 , verbose=1\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae0312",
   "metadata": {},
   "source": [
    "In the following the best parameters of the grid search are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719c8784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd781ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893496a",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "It is generally not recommended to use the R^2 metric to evaluate the performance of a random forest model, because the R^2 metric is not well-suited for evaluating the performance of models that do not make predictions using a linear function. Instead, it is generally better to use error metrics that are more appropriate for non-linear models, such as mean squared error (MSE) or mean absolute error (MAE).\n",
    "\n",
    "In terms of the MAE, MSE, and RMSE values, a lower value generally indicates better performance. So, a MAE of 16.07, MSE of 677.48, and RMSE of 26.03 are considered to be relatively good performance values. The same applies for MAPE (21.12%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfdd0e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 16.07\n",
      "MSE: 677.48\n",
      "MAPE: 21.12%\n",
      "RMSE: 26.03\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model based on the best parameters of the grid search\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.2f}\")\n",
    "print(f\"MAPE: {(mean_absolute_error(y_test, y_pred) / y_test.mean()) * 100:.2f}%\")\n",
    "print(f\"RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6660ae8",
   "metadata": {},
   "source": [
    "Outlook: We might be able to improve the performance of Random Forest by increasing the options of the parameters in the grid search. This is more computationally intensive and takes more time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b69fa0ca125f7a75e8642045dca3cc5814dc978d434d8649da025a3b8baa816"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
